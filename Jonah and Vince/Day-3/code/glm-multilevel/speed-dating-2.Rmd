---
title: "Playing with Speed Dating Data Pt 2: MLM"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
par(mar = c(2.5, 2.5, 2, 0.1), mgp = c(1.5, 0.3, 0))
```

```{r load-packages, message=FALSE, warning=FALSE, echo=FALSE}
library(rstan)
options(mc.cores = parallel::detectCores())
```

## Loading the Data

This time around we're going to fit a multilevel model that includes a random intercept for each subject. These (latent) parameters measure each participants' a natural baseline of "pickiness" that determines their willingness to date. To keep things simple, we'll fit a separate model to the male and female subjects, but the model could definitely be made more complex and their estimates pooled.

```{r load-data}
speed_dating <- read.csv("speed_dating.csv.gz")

male_data   <- subset(speed_dating, isMale == 1)
female_data <- subset(speed_dating, isMale == 0)
```

## Stan Model

We're going to start with a simple Stan model for a generalized linear, hierarchical model for Bernoulli data using a logistic link function and contains only a varying intercept for each individual. (The html output appears to be missing a bit due to some rendering errors, check the source.)

```{r engine='cat', engine.opts=list(file = "mlm_logit.stan", lang = "stan")}
data {
  int<lower = 0> N;
  int<lower = 0> K;
  int<lower = 0> N_groups; // num groups
  
  int<lower = 0, upper = 1> y[N];
  int<lower = 1, upper = N_groups> group[N];
  matrix[N,K] x;
}
parameters {
  vector[K] beta;
  
  vector[N_groups] alpha;
  real<lower = 0> sigma_alpha;
}
model {
  y ~ bernoulli_logit(x * beta + alpha[group]);
  alpha ~ normal(0, sigma_alpha);
  
  beta[1] ~ cauchy(0, 10);
  beta[2:K] ~ cauchy(0, 2.5);
  sigma_alpha ~ cauchy(0, 10);
}
```

## Preparing Data

Next up, we extract data from the data frame in a format that Stan can use (i.e. a matrix). This step is a little more complicated than it needs to be, as the code generalizes nicely to more complicated models with interactions. If we had more than one grouping factor, we can something like `glmer` in the `lme4` package to do this for us, or more realistically we can just fit a Bayesian version directly by using `rstanarm`.

```{r data-preparation}
mf <- model.frame(wouldDate ~ 0 + attractiveness + sincerity +
                  intelligence + fun + ambition + sharedInterest +
                  studentId, male_data)

y <- model.response(mf)
x <- model.matrix(attr(mf, "terms"), mf)
group <- x[,"studentId"]

data <- list(y = y,
             ## remove studentId from x, add a column of 1s
             x = cbind(1, x[,-which(colnames(x) == "studentId")]),
             ## shift g so that it operates as indices (starts at 1, ...)
             group = match(group, unique(group)),
             N = length(y),
             K = ncol(x), ## -1 for studentId, +1 for intercept
             N_groups = length(unique(group)))
```

Our model first appears in the call to `model.frame`, that is we are going to estimate

$$ P(\text{wouldDate}_{ij} = 1 \mid \cdot) = \mathrm{logit}^{-1}(\text{intercept} + \text{attractiveness}_{ij} + \cdots + \text{intercept for student}_j) $$

where we now need a $j$ index to go through individuals.

## Fitting the Model

```{r model-fitting}
male_fit <- stan("mlm_logit.stan", data = data)
print(male_fit, pars = "beta")
```

## Varying Intercepts

`beta` is as in a non-hierarchical generalized linear model, namely the baseline "intercept" and the slopes contributed by the various predictors. `alpha` are the offsets for each student. Occasionally, the varying slopes are "nuisance parameters" and are simply added to account for variation at a group level. Often, they are of interest themselves. Here, we examine them directly.

```{r looking-at-intercepts}
alpha_rep <- as.matrix(male_fit, pars = "alpha")
dim(alpha_rep)
data$N_group
```

We can examine the samples of any one person's offset by indexing into `alpha_rep`. For example, the marginal distributions of the first 6 students' is shown by:

```{r looking-at-intercepts-2}
par(mfrow = c(2, 3))
for (i in 1:6)
  hist(alpha_rep[,i], xlim = range(alpha_rep[,1:6]), breaks = 8,
       main = paste0("Post Dist Offset Student ", i), xlab = "alpha")
```

## Exercise: Intercept Posterior Distributions

It looks as if there is a fair bit of variation in students' baseline preferences. This can be visualized by summarizing the marginal posteriors of all `r data$N_groups` offsets and calculating their posterior means, which have their own distribution distribution. Plot a histogram of these means:

```{r posterior-means-dist}
alpha_means <- apply(alpha_rep, 2, mean)
length(alpha_means) ## should be equal number of students
## ...
```

Further, the 6 distributions above are all *marginal* posteriors. Modify the histogram code to create scatter plots of the joint posteriors between $\alpha_1$ and $\alpha_2$ through $\alpha_7$.

```{r posterior-scatter-plots}
par(mfrow = c(2, 3))
for (i in 2:7)
  ## plot(alpha_rep[,1],
  ## ...
  {} ## empty braces just to get file to compile, replace later
```

## Predictions for Individuals

If we have a specific individual in mind, making predictions for that individual is accomplished by adding their offset to the linear predictor, composed when making predictions in non-hierarchical multilevel models. In that case, as in here, we need to setup a vector that represents the point at which we want predictions. We start by considering a collection of individuals who are otherwise completely average but differ in their attractiveness.

```{r posterior-predictions}
beta_rep <- as.matrix(male_fit, pars = "beta")

xRange <- range(male_data$attractiveness)
n.vals <- 101
x.predict <- colMeans(data$x)
xVals <- seq(xRange[1], xRange[2], length.out = n.vals)

## do everything in matrices this time

## this duplicates the x.predict vector into a matrix with
## identical columns; check by doing x.predict[,1:3]
x.predict <- matrix(x.predict, length(x.predict), n.vals)
rownames(x.predict) <- colnames(data$x)

## set the attractiveness row of x.predict to our new values
x.predict["attractiveness",] <- xVals

## x.predict should be 7 x 101, beta_rep is 4000 x 7, so
lin.pred_rep <- beta_rep %*% x.predict

## now add offset for first person, will get added as a column
## to each column
lin.pred_rep <- lin.pred_rep + alpha_rep[,1]

## now convert to probabilities and take an average
probs <- apply(plogis(lin.pred_rep), 2, mean)

plot(male_data$attractiveness, y, pch = 20,
     xlab = "attractiveness", ylab = "Prob Would Date",
     main = "Student 1")
lines(xVals, probs)
```

This shows just the posterior mean for the first individual, but not the uncertainty in the estimation. Modify the above to plot 20 lines in gray corresponding to 20 of the posterior samples, as well as the posterior mean.

```{r posterior-predictive-uncertainty}
## lin.pred_rep remains valid, what is required is to calculate 20 sets of probs and
## use those to plot 20 sets of lines

## ...
```

## Further Exercises

The above exercise shows just the variability in the fit for first student. Plotting a number of these side-by-side illustrates the variation between and within students all at once. Also of interest are predictions for *new* students. These can be obtained by taking draws from the distribution

$$p(\alpha^* \mid y) = \int p(\alpha^* \mid \sigma_\alpha) p(\sigma_\alpha \mid y) \,\mathrm{d}\alpha$$

which is done by using a sample of `sigma_alpha` (from $p(\sigma_\alpha \mid y)$) and simulating a normal random variable with the standard deviation ($p(\alpha^* \mid \sigma_\alpha)$). More details can be found in BDA, or [Gelman and Hill](http://www.stat.columbia.edu/~gelman/arm/) (2006).

Ultimately, the model is most interesting when fit with varying intercepts and varying slopes, as well as including interactions between the predictors. For example, the interaction between ambition and attractiveness is negatively correlated in men's preferences, but non-significant in women's.